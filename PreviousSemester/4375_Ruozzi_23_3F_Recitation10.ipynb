{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recitation 10\n",
        "\n",
        "UTDallas CS 4375, taught by Dr. Ruozzi\n",
        "\n",
        "Recitation with Jim Amato"
      ],
      "metadata": {
        "id": "3Rq5EX_0N9Wy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Jim notes to self for imaginary eventual update:"
      ],
      "metadata": {
        "id": "0R-COnu4LQRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Better math. That was one of the two big points of confusion:\n",
        "  * What does the math in the slides mean?\n",
        "  * How does it translate to the stuff in this notebook?\n",
        "* Make \"Why so weird looking\" a footnote with a link\n",
        "* Store test/train data in a class?\n",
        "* Store eigenstuff in a class?\n",
        "* Have PCA Classifier class, input is just k (# eigenvectors to use)\n",
        "* Have visualization tools that operate on that class to show eigenvectors, centroids, prototypes\n",
        "* Revisit part 4; organize math there better so it meshes better with the code\n",
        "* Rearrange starting around (7)\n",
        "  * Classification results with k=2 (as currently exists)\n",
        "  * Visualizations here?\n",
        "  * Then go to k=3,\n",
        "  * More visualizations; compare 2 to 3?\n",
        "  * Then all k from 2-20ish\n",
        "  * Plot of train acc and test acc\n",
        "  * Then sidebar 1: Wiggle Room (top n results)\n",
        "  * Last sidebar 2: Why so weird looking footnote (comparing this toy to full MNIST)\n",
        "* Add 3D Visualization tool\n",
        "* Widget that allows changing k and shows eigenvectors/centroids/prototypes ?\n",
        "* Add Q's & A's throughout?\n",
        "* Other extensions to this? Using PCA with X method?\n",
        "* Merge with W9? Or better balance placement of math stuff (between the two)?\n",
        "  * Flesh out math stuff in here?\n",
        "* Provide templates to be filled out while going over this (instead of completed code)\n",
        "  * SubtractMeanScaler\n",
        "  * EigenStuff\n",
        "  * Centroids\n",
        "  * PCA Classifier (dot products vs multiplication for projection)"
      ],
      "metadata": {
        "id": "-EB5GpkfHPF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{blue}{\\text{Today:}}$"
      ],
      "metadata": {
        "id": "vwqhgtf-OBLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Any requests?\n",
        "* PCA continued: PCA on Fun Data"
      ],
      "metadata": {
        "id": "mqa7JCisOSTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{blue}{\\text{Problem 1:}} \\text{ PCA on MNIST}$"
      ],
      "metadata": {
        "id": "D6yjHD73R5Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{1.}} \\text{ Quick PCA Review}$"
      ],
      "metadata": {
        "id": "x-r_QsxIrL1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\<Refer to last week's Colab notebook\\>"
      ],
      "metadata": {
        "id": "Zd3ISOcRrV1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{2.}} \\text{ MNIST}$"
      ],
      "metadata": {
        "id": "iWuKaluardOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MNIST dataset is literally everywhere. You've certainly seen it. It's a default (\"toy\") dataset in scikit-learn. More information is available:\n",
        "* In [Scikit-Learn's Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)\n",
        "* In a [Scikit-Learn Tutorial](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py)\n",
        "* At the [UCI Archive](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits)"
      ],
      "metadata": {
        "id": "gAjz013MrhzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we load the MNIST Dataset and split into test and training sets."
      ],
      "metadata": {
        "id": "-jz90DeUosR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "digits = load_digits()\n",
        "print(digits.data.shape)\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "print(f\"Number of features per sample: {X.shape[1]}\")\n",
        "print(f\"Unique classes in the dataset: {np.unique(y)}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Ns2JzUm1ornm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at some examples."
      ],
      "metadata": {
        "id": "1tJ_Th_IpLWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a grid of sample digits\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
        "for ax, image, label in zip(axes.ravel(), digits.images, digits.target):\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
        "    ax.set_title(f'Label: {label}')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VqQ47ujUpXWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{3.}} \\text{ Normalization}$"
      ],
      "metadata": {
        "id": "gXCiotEayvQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing data, especially in the context of PCA, is essential. For PCA, we often ensure our data has zero mean.\n",
        "\n"
      ],
      "metadata": {
        "id": "vHXQTfuPywX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is in the slides on slide 11. We construct the matrix $W\\in\\mathbb{R}^{n\\times p}$ whose $i^\\text{th}$ column is:\n",
        "$$ x_i - \\frac{\\sum_j x_j}{p}$$\n",
        "\n",
        "I'm going to deviate a bit and have my data in rows (like usual) instead."
      ],
      "metadata": {
        "id": "B14jI1Jk0tAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanSubtractionScaler:\n",
        "    def __init__(self, X):\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "\n",
        "    def scale(self, X):\n",
        "        return X - self.mean\n",
        "\n",
        "    def unscale(self, s_X):\n",
        "        return s_X + self.mean"
      ],
      "metadata": {
        "id": "TWug_CQty_LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MeanSubtractionScaler(X_train)\n",
        "X_train_scaled = scaler.scale(X_train)\n",
        "X_test_scaled = scaler.scale(X_test)"
      ],
      "metadata": {
        "id": "mygV8ntUzB3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let's look at what happens when we subtract the mean from our images."
      ],
      "metadata": {
        "id": "u_TghQx70w_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axarr = plt.subplots(1, 1, figsize=(15, 3))\n",
        "\n",
        "for i in range(1):\n",
        "    # Original images\n",
        "    axarr.imshow(scaler.mean.reshape(8, 8), cmap='gray_r')\n",
        "    axarr.axis('off')\n",
        "    axarr.set_title(f'Mean Digit')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZuJuCnxZR50e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axarr = plt.subplots(2, 5, figsize=(15, 6))\n",
        "\n",
        "# Display 5 original and mean-subtracted images\n",
        "for i in range(5):\n",
        "    # Original images\n",
        "    axarr[0, i].imshow(X_train[i].reshape(8, 8), cmap='gray_r')\n",
        "    axarr[0, i].axis('off')\n",
        "    axarr[0, i].set_title(f'Original Image {i+1}')\n",
        "\n",
        "    # Mean-subtracted images\n",
        "    axarr[1, i].imshow(X_train_scaled[i].reshape(8, 8), cmap='gray_r', vmin=-16, vmax=16)\n",
        "    axarr[1, i].axis('off')\n",
        "    axarr[1, i].set_title(f'Mean-Subtracted Image {i+1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uyz136W-z1u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They only sorta look like numbers now. However, this ensures that our principle components are truly the directions of maximum ***variance***."
      ],
      "metadata": {
        "id": "8UY1IXSK070U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{4.}} \\text{ Eigenvectors}$"
      ],
      "metadata": {
        "id": "5Bf1HOSC0E3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use the SVD method, meaning we don't compute the covariance matrix and use the data directly."
      ],
      "metadata": {
        "id": "caqaSFLs0LEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_eigen_via_svd(X):\n",
        "    # Compute the SVD\n",
        "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
        "\n",
        "    # The eigenvalues are the squared singular values\n",
        "    eigenvalues = s**2\n",
        "\n",
        "    # The eigenvectors are the columns of Vt\n",
        "    eigenvectors = Vt\n",
        "\n",
        "    # Sort the eigenvalues and eigenvectors by descending eigenvalue\n",
        "    sort_indices = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[sort_indices]\n",
        "    eigenvectors = eigenvectors[:, sort_indices]\n",
        "\n",
        "    return eigenvalues, eigenvectors\n",
        "\n",
        "eigenvalues, eigenvectors = compute_eigen_via_svd(X_train_scaled)"
      ],
      "metadata": {
        "id": "hnqgLKtj2GBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is exactly what we did last week."
      ],
      "metadata": {
        "id": "fc0b8t6Y2YlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now going to get $Q_k$, the top $k$ eigenvectors, as described on slide 9 this way:\n",
        "\n",
        "The Frobenius norm is a matrix norm given by\n",
        "$$ ||A||_F=\\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n|A_{ij}|^2}$$\n",
        "\n",
        "$Q_kD_kQ_K^\\text{T}$ is the best rank $k$ approximation of the symetric matrix $A$ with respect to the Frobenius norm\n",
        "$$Q_kD_kQ_k^\\text{T} = \\underset{B\\in\\mathbb{R}^{n\\times n}\\text{s.t.}\\text{rank}(B)=k}{\\mathrm{argmin}}||A-B||_F$$"
      ],
      "metadata": {
        "id": "CJQgb_0i2m-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's scary looking math, but it just means that if our data $\\mathbf{X}$ has $d$ dimensions ($\\mathbf{x}\\in\\mathbb{R}^d$) then $Q_k$ is going to be a matrix of size $d \\times k$:\n",
        "$$Q_k = \\begin{bmatrix}\\mathbf{v}_1 & \\mathbf{v}_2 & \\dots & \\mathbf{v}_k\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "L7RMGLRn4bUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Qk(k_):\n",
        "    return eigenvectors[:k_]\n",
        "\n",
        "Q_2 = get_Qk(2)"
      ],
      "metadata": {
        "id": "m-zf3YXb2mQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{5.}} \\text{ Projecting Data}$"
      ],
      "metadata": {
        "id": "yVAolXNG2b2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a new space into which we can project our data and get a more simple representation of it. We do this simply with the dot product."
      ],
      "metadata": {
        "id": "alKsBDKS52E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_projected = np.dot(X_train_scaled, Q_2.T)\n",
        "X_test_projected = np.dot(X_test_scaled, Q_2.T)"
      ],
      "metadata": {
        "id": "v2SxGicR6DTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What did the data look like before? What does it look like in the new space?"
      ],
      "metadata": {
        "id": "PPtMa8Si6Ste"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"One datum in the old space:\")\n",
        "print(X_train[0])\n",
        "print(\"\\nOne datum in the new space:\")\n",
        "print(X_train_projected[0])"
      ],
      "metadata": {
        "id": "u9xqprcf6cAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can much more easily imagine data points in this new space. Let's plot several."
      ],
      "metadata": {
        "id": "t0r__APg6pBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "colors = sns.color_palette('hsv', 10)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for digit, color in enumerate(colors):\n",
        "    # Extract data points of the current class\n",
        "    indices = y_train == digit\n",
        "    plt.scatter(X_train_projected[indices, 0],\n",
        "                X_train_projected[indices, 1],\n",
        "                color=color,\n",
        "                s=50, alpha=0.6, label=str(digit))\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Projection (2D) of Digits Data')\n",
        "plt.legend(title='Digits', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tC543b2_7HEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{6.}} \\text{ Centroids}$"
      ],
      "metadata": {
        "id": "nX_0lyFOGqOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great. We can imagine a point in the middle of any of those blobs of color. That's the ***centroid***. We're going to determine the centroid for each of these classes."
      ],
      "metadata": {
        "id": "DMSdh1N17GOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centroids = {}\n",
        "for i in range(10):  # since there are 10 classes (0 through 9)\n",
        "    centroids[i] = np.mean(X_train_projected[y_train == i], axis=0)\n",
        "\n",
        "for digit, centroid in centroids.items():\n",
        "    print(f\"Centroid for digit {digit}: {centroid}\")"
      ],
      "metadata": {
        "id": "ns6GPq0Q7mqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plotting the PCA-Projected Data\n",
        "for digit, color in enumerate(colors):\n",
        "    # Extract data points of the current class\n",
        "    indices = y_train == digit\n",
        "    plt.scatter(X_train_projected[indices, 0],\n",
        "                X_train_projected[indices, 1],\n",
        "                color=color,\n",
        "                s=50, alpha=0.6, label=str(digit))\n",
        "\n",
        "# Plotting the Centroids\n",
        "for digit, centroid in centroids.items():\n",
        "    plt.scatter(centroid[0],\n",
        "                centroid[1],\n",
        "                color=colors[digit],\n",
        "                marker='s', edgecolor='black',\n",
        "                linewidth=1, s=200, label=f\"Centroid {digit}\")\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Projection (2D) of Digits Data with Centroids')\n",
        "plt.legend(title='Digits', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0ECvXijc8i5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{7.}} \\text{ Classification}$"
      ],
      "metadata": {
        "id": "ALbA-EAA8Axt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can try to classify data based on its location. We're going to simply take the projected data and find the centroid closest to it. There are certainly more advanced methods than this (you'll use one on the next homework), but it's a great intuitive start."
      ],
      "metadata": {
        "id": "FMKJB3kc8Cgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def centroid_class(X, centroids):\n",
        "    centroid_np = np.array(list(centroids.values()))\n",
        "    diff = X[:, np.newaxis] - centroid_np\n",
        "    data_distances = np.linalg.norm(diff, axis=2)\n",
        "    predicted_classes = np.argsort(data_distances, axis=1)\n",
        "    return predicted_classes[:, :1]\n",
        "\n",
        "def is_label_the_top_prediction(y_true, y_pred):\n",
        "    matches = (y_true[:, np.newaxis] == y_pred)\n",
        "    return np.any(matches, axis=1)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    matches = (y_true[:, np.newaxis] == y_pred)\n",
        "    correct = np.sum(np.any(matches, axis=1))\n",
        "    total = y_true.shape[0]\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "J3me9zIX9M-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = centroid_class(X_train_projected, centroids)\n",
        "y_test_pred = centroid_class(X_test_projected, centroids)\n",
        "\n",
        "print(f\"On the training set, we get: {accuracy(y_train, y_train_pred)*100:.01f}% correct\")\n",
        "print(f\"On the testing set, we get: {accuracy(y_test, y_test_pred)*100:.01f}% correct\")"
      ],
      "metadata": {
        "id": "kTXXaZ2i9bJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's not bad at all! We'd get 10% with random guessing."
      ],
      "metadata": {
        "id": "LQ0V8k5L-ENR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{7.1}} \\text{ Experiment: More Eigenvectors}$"
      ],
      "metadata": {
        "id": "hlXTGo1j8BiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did that with only the top two eigenvectors. What happens if we use more? Let's try 3."
      ],
      "metadata": {
        "id": "VSwcE2sD-s9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q_3 = get_Qk(3)\n",
        "\n",
        "X_train_projected_3 = np.dot(X_train_scaled, Q_3.T)\n",
        "X_test_projected_3 = np.dot(X_test_scaled, Q_3.T)\n",
        "\n",
        "centroids_3 = {}\n",
        "for i in range(10):  # since there are 10 classes (0 through 9)\n",
        "    centroids_3[i] = np.mean(X_train_projected_3[y_train == i], axis=0)\n",
        "\n",
        "for digit, centroid in centroids_3.items():\n",
        "    print(f\"Centroid for digit {digit}: {centroid}\")"
      ],
      "metadata": {
        "id": "lzeLvR5g-zzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = centroid_class(X_train_projected_3, centroids_3)\n",
        "y_test_pred = centroid_class(X_test_projected_3, centroids_3)\n",
        "\n",
        "print(f\"On the training set, we get: {accuracy(y_train, y_train_pred)*100:.01f}% correct\")\n",
        "print(f\"On the testing set, we get: {accuracy(y_test, y_test_pred)*100:.01f}% correct\")"
      ],
      "metadata": {
        "id": "VySJED_q_5mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Way better!\n",
        "\n",
        "How many eigenvectors should we use? We can look at our eigenvalues to see where they drop off."
      ],
      "metadata": {
        "id": "U3WNxqVdBhRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(eigenvalues, '-o')\n",
        "plt.title('Eigenvalues of the Covariance Matrix')\n",
        "plt.xlabel('Eigenvalue Index')\n",
        "plt.ylabel('Eigenvalue (log scale)')\n",
        "plt.ylim(bottom=1)\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oae_w1H6BnnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's on a log scale, where we see a pretty smooth fall off after the first 4. So I think 4 is probably good. Let's check performance."
      ],
      "metadata": {
        "id": "RuGkwCf8CLet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "def many_q_k():\n",
        "    res = []\n",
        "    for i in range(2, 20):\n",
        "        Q_k = get_Qk(i)\n",
        "\n",
        "        X_train_projected_k = np.dot(X_train_scaled, Q_k.T)\n",
        "        X_test_projected_k = np.dot(X_test_scaled, Q_k.T)\n",
        "\n",
        "        centroids_k = {}\n",
        "        for j in range(10):  # since there are 10 classes (0 through 9)\n",
        "            centroids_k[j] = np.mean(X_train_projected_k[y_train == j], axis=0)\n",
        "\n",
        "        y_train_pred = centroid_class(X_train_projected_k, centroids_k)\n",
        "        y_test_pred = centroid_class(X_test_projected_k, centroids_k)\n",
        "\n",
        "        res.append(dict(k=i,\n",
        "                        train_acc=accuracy(y_train, y_train_pred),\n",
        "                        test_acc=accuracy(y_test, y_test_pred)))\n",
        "\n",
        "    print(tabulate(res, headers=\"keys\"))\n",
        "many_q_k()"
      ],
      "metadata": {
        "id": "8DTfFKtlCXC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{7.2}} \\text{ Experiment: More Wiggle Room}$"
      ],
      "metadata": {
        "id": "bB9Gwvxc-ZMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WIP"
      ],
      "metadata": {
        "id": "z7kmn5PkGdwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def centroid_class(X, centroids, first_n):\n",
        "#     centroid_np = np.array(list(centroids.values()))\n",
        "#     diff = X[:, np.newaxis] - centroid_np\n",
        "#     data_distances = np.linalg.norm(diff, axis=2)\n",
        "#     predicted_classes = np.argsort(data_distances, axis=1)\n",
        "#     return predicted_classes[:, :first_n]\n",
        "\n",
        "# def is_label_in_top_predictions(y_true, y_pred):\n",
        "#     matches = (y_true[:, np.newaxis] == y_pred)\n",
        "#     return np.any(matches, axis=1)\n",
        "\n",
        "# def accuracy(y_true, y_pred):\n",
        "#     matches = (y_true[:, np.newaxis] == y_pred)\n",
        "#     correct = np.sum(np.any(matches, axis=1))\n",
        "#     total = y_true.shape[0]\n",
        "#     return correct / total"
      ],
      "metadata": {
        "id": "w7pU40Yy9i1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quPEtuFoHD8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{8.1}} \\text{ Visualization: Eigenvectors}$"
      ],
      "metadata": {
        "id": "-yZnFV198Qox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_eigenvectors = 3\n",
        "Q_u = None\n",
        "X_train_projected_u = None\n",
        "X_test_projected_u = None\n",
        "centroids_u = None\n",
        "\n",
        "def update_eigenstuff(num_eigenvectors = 10):\n",
        "    global Q_u, X_train_projected_u, X_test_projected_u, centroids_u\n",
        "    Q_u = get_Qk(num_eigenvectors)\n",
        "\n",
        "    X_train_projected_u= np.dot(X_train_scaled, Q_u.T)\n",
        "    X_test_projected_u= np.dot(X_test_scaled, Q_u.T)\n",
        "\n",
        "    centroids_u= {}\n",
        "    for i in range(10):  # since there are 10 classes (0 through 9)\n",
        "        centroids_u[i] = np.mean(X_train_projected_u[y_train == i], axis=0)"
      ],
      "metadata": {
        "id": "odRTCO4CF8ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_eigenstuff(n_eigenvectors)"
      ],
      "metadata": {
        "id": "As3Dg2inbKyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure and subplots\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
        "\n",
        "for i, ax in enumerate(axes.ravel()):\n",
        "    if i < n_eigenvectors:\n",
        "        # temp = Q_u[i] + scaler.mean\n",
        "        eigenimage = Q_u[i].reshape(8, 8)\n",
        "        ax.imshow(eigenimage, cmap='gray_r')\n",
        "        ax.set_title(f'Eigenvector {i+1}')\n",
        "    ax.axis('off')  # Turn off axis numbers and ticks\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9LDpVCOzHKoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{8.2}} \\text{ Visualization: Centroids}$"
      ],
      "metadata": {
        "id": "KDgpMvow8RUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
        "\n",
        "for digit, ax in zip(centroids, axes.ravel()):\n",
        "    # Project centroid back to the original space\n",
        "    projected_centroid = np.dot(centroids_u[digit], Q_u)\n",
        "    projected_centroid = scaler.unscale(projected_centroid)\n",
        "    centroid_image = projected_centroid.reshape(8, 8)\n",
        "    ax.imshow(centroid_image, cmap='gray_r')\n",
        "    ax.set_title(f'Digit {digit}')\n",
        "    ax.axis('off')  # Turn off axis numbers and ticks\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6eWmSwRCF8BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
        "\n",
        "for digit, ax in zip(centroids, axes.ravel()):\n",
        "    # Project centroid back to the original space\n",
        "    projected_centroid = np.dot(centroids_u[digit], Q_u)\n",
        "    projected_centroid = scaler.unscale(projected_centroid)\n",
        "    centroid_image = projected_centroid.reshape(8, 8)\n",
        "    ax.imshow(centroid_image, cmap='gray_r')\n",
        "    ax.set_title(f'Digit {digit}')\n",
        "    ax.axis('off')  # Turn off axis numbers and ticks\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BlDYs3ELovUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{8.3}} \\text{ Visualization: Prototypes}$"
      ],
      "metadata": {
        "id": "W6QzMZb08Zp0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k97I4NckF7fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{9}} \\text{ Wrap-Up and Extra}$"
      ],
      "metadata": {
        "id": "nP98xZ5iF0zB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tbd"
      ],
      "metadata": {
        "id": "xdgeudIrF5Vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\color{blue}{\\text{2.}} \\text{ Sidebar: Why so weird looking?}$"
      ],
      "metadata": {
        "id": "tjYE30McwOB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They look... recognizable, but also a bit weird. Why is that? Here's the description available at [UCI](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits):\n",
        "\n",
        "\"We used preprocessing programs made available by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of $43$ people, $30$ contributed to the training set and different $13$ to the test set. $32\\times32$ bitmaps are divided into nonoverlapping blocks of $4\\times4$ and the number of on pixels are counted in each block. This generates an input matrix of $8\\times8$ where each element is an integer in the range $[0\\dots16]$. This reduces dimensionality and gives invariance to small distortions.\"\n",
        "\n",
        "(This isn't the full MNIST, which is 32x32 pixel images, this is the toy dataset from Scikit-Learn.)"
      ],
      "metadata": {
        "id": "20QuQ9u5uJm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Image URL\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png\"\n",
        "\n",
        "# Fetch the image using requests\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Make sure the request was successful\n",
        "response.raise_for_status()\n",
        "\n",
        "local_image_filename = \"local_mnist.png\"\n",
        "\n",
        "# Save the image locally\n",
        "with open(local_image_filename, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"Image downloaded and saved as local_image.png\")"
      ],
      "metadata": {
        "id": "W7eUjhdtxCVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Replace this path with the path to your image\n",
        "image = Image.open(local_image_filename)\n",
        "\n",
        "# Define the left, upper, right, and lower pixel coordinates for cropping\n",
        "# This will extract the top-left 28x28 pixels\n",
        "cropped_image = image.crop((0, 0, 28, 28))\n",
        "\n",
        "# Convert to grayscale\n",
        "image_gray = cropped_image.convert('L')\n",
        "threshold = 128\n",
        "image_binary = image_gray.point(lambda p: 255 if p > threshold else 0)\n",
        "\n",
        "# Process the image to get the 7x7 matrix\n",
        "matrix_7x7 = np.zeros((7, 7))\n",
        "\n",
        "for i in range(0, 28, 4):\n",
        "    for j in range(0, 28, 4):\n",
        "        # Extract the 4x4 block from the image\n",
        "        block = image_binary.crop((j, i, j+4, i+4))\n",
        "\n",
        "        # Count the number of 'on' pixels. We'll consider a pixel 'on' if its value is below 128 (midway in 0-255 scale)\n",
        "        on_pixel_count = np.sum(np.array(block) < 128)\n",
        "\n",
        "        # Store the count in the 8x8 matrix\n",
        "        matrix_7x7[i//4, j//4] = on_pixel_count\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(8, 4))\n",
        "ax1.imshow(cropped_image)\n",
        "ax1.set_title('Original Image')\n",
        "ax1.axis('off')\n",
        "\n",
        "ax2.imshow(image_binary, cmap='gray')\n",
        "ax2.set_title('Black and White Image')\n",
        "ax2.axis('off')\n",
        "\n",
        "ax3.imshow(matrix_7x7, cmap='gray_r', vmin=0, vmax=16)\n",
        "ax3.set_title('Processed 7x7 Representation')\n",
        "ax3.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4W5rSFGdwU9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\text{Template}$"
      ],
      "metadata": {
        "id": "Gt36iezgz0Zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\text{Completed}$"
      ],
      "metadata": {
        "id": "bRBWOg2CzwlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\text{Showcase Code}$"
      ],
      "metadata": {
        "id": "Q2frx9T8z60w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\text{Showcase}$"
      ],
      "metadata": {
        "id": "z5rgrqTezv2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources"
      ],
      "metadata": {
        "id": "EhNFnWJ_LQFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2J3nkg3EohAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "digits = load_digits()\n",
        "print(digits.data.shape)\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# display_digit_idx = 2\n",
        "# display_digit = X[0].reshape(8, 8)\n",
        "# plt.imshow(display_digit, cmap=\"gray_r\")\n",
        "# plt.show()\n",
        "\n",
        "class OurOwnScaler:\n",
        "    def __init__(self, X):\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "\n",
        "    def scale(self, X):\n",
        "        return X - self.mean\n",
        "\n",
        "    def unscale(self, s_X):\n",
        "        return s_X + self.mean\n",
        "\n",
        "scaler = OurOwnScaler(X_train)\n",
        "X_tr_sc = scaler.scale(X_train)\n",
        "X_te_sc = scaler.scale(X_test)\n",
        "\n",
        "U, s, VT = np.linalg.svd(X_tr_sc, full_matrices=False)\n",
        "\n",
        "k = 3\n",
        "\n",
        "top_k_vectors = VT[:k]\n",
        "\n",
        "X_train_projected = np.dot(X_tr_sc, top_k_vectors.T)\n",
        "\n",
        "centroids = {}\n",
        "for i in range(10):  # since there are 10 classes (0 through 9)\n",
        "    centroids[i] = np.mean(X_train_projected[y_train == i], axis=0)\n",
        "\n",
        "for digit, centroid in centroids.items():\n",
        "    print(f\"Centroid for digit {digit}: {centroid}\")\n",
        "\n",
        "X_test_projected = np.dot(X_te_sc, top_k_vectors.T)\n",
        "\n",
        "def prototype_class(X, centroids, first_n):\n",
        "    centroid_np = np.array(list(centroids.values()))\n",
        "    diff = X[:, np.newaxis] - centroid_np\n",
        "    data_distances = np.linalg.norm(diff, axis=2)\n",
        "    predicted_classes = np.argsort(data_distances, axis=1)\n",
        "    return predicted_classes[:, :first_n]\n",
        "\n",
        "h_first_n = 2\n",
        "y_train_pred = prototype_class(X_train_projected, centroids, h_first_n)\n",
        "y_test_pred = prototype_class(X_test_projected, centroids, h_first_n)\n",
        "\n",
        "def is_label_in_top_predictions(y_true, y_pred):\n",
        "    matches = (y_true[:, np.newaxis] == y_pred)\n",
        "    return np.any(matches, axis=1)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    matches = (y_true[:, np.newaxis] == y_pred)\n",
        "    correct = np.sum(np.any(matches, axis=1))\n",
        "    total = y_true.shape[0]\n",
        "    return correct / total\n",
        "\n",
        "# in_top_predictions = is_label_in_top_predictions(y_test, y_test_pred)\n",
        "# in_top_predictions = is_label_in_top_predictions(y_train, y_train_pred)\n",
        "\n",
        "# Count how many are correctly classified within the top 'first_n' predictions\n",
        "print(accuracy(y_train, y_train_pred))\n",
        "print(accuracy(y_test, y_test_pred))\n",
        "\n",
        "# # Number of eigenvectors to display\n",
        "# num_eigenvectors = 10\n",
        "\n",
        "# # Create a figure and subplots\n",
        "# fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
        "\n",
        "# for i, ax in enumerate(axes.ravel()):\n",
        "#     if i < num_eigenvectors:\n",
        "#         eigenimage = top_k_vectors[i].reshape(8, 8)\n",
        "#         ax.imshow(eigenimage, cmap='gray')\n",
        "#         ax.set_title(f'Eigenvector {i+1}')\n",
        "#     ax.axis('off')  # Turn off axis numbers and ticks\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # Number of centroids (should be 10 for the digits dataset)\n",
        "# num_centroids = len(centroids)\n",
        "\n",
        "# # Create a figure and subplots\n",
        "# fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
        "\n",
        "# for digit, ax in zip(centroids, axes.ravel()):\n",
        "#     # Project centroid back to the original space\n",
        "#     projected_centroid = np.dot(centroids[digit], top_k_vectors)\n",
        "#     projected_centroid = scaler.unscale(projected_centroid)\n",
        "#     centroid_image = projected_centroid.reshape(8, 8)\n",
        "#     ax.imshow(centroid_image, cmap='gray_r')\n",
        "#     ax.set_title(f'Digit {digit}')\n",
        "#     ax.axis('off')  # Turn off axis numbers and ticks\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "iBkOX5mMDvqI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}